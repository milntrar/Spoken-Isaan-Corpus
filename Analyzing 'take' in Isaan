#This project investigates the uses of ʔaw 'take' in different clause types by analysizing concordances
#Research question: what verb class co-occur with ʔaw in serial verb clauses?
#I propose that ʔaw + deictic verb is to some degree lexicalized. This can be further investigated by performing a separate collocation analysis

#importing Isaan corpus text in XML format 

import xml.etree.ElementTree as ET #import ElementTree

tree = ET.parse("Isaan_spoken_corpus_2020.xml")
root = tree.getroot()
corpus_text = ET.tostring(root, encoding ='utf8').decode('utf8')

#extracting relevant information using the following function

def Isaan_xml_dicter(text, splitter = None): #splitter is not used - it is only here to conform with other tokenizer functions so it will work with corpus_freq()
	out_list = [] ##list for each token
	root = ET.fromstring(text) #starting point in parse tree
	
	for x in root.iter(tag = "iword"): #iterate through the "iword" tags(Thai script)
		token = {}
		for z in x.iter(tag = "item"):#iterate through another tag
		#in this format, spaces are included in the "item" tag text.
			word = z.text #get word text

			if word == None: #in some cases, <item> tags don't have text.
				continue #we will ignore these
			else:
				word = word.strip() #remove trailing spaces
			
			if z.get("lang")=="tts-Thai-baseline":#if the attribute "lang" is "tts-Thai-baseline" (Isaan words but Thai script)
				token["word"] = word

			elif z.get("lang") == "en-wordGloss":#if the attribute "lang" is "en-wordGloss" (Englis gloss)
				token["gloss"] = word
			
			elif z.get("lang")== "en-wordCategory":#if you want part of speech as well
				token["pos"] = word
				
			else:#if the attribute "lang" is something else
				continue#ignore then
				
		if "word" not in token:#if the word (key) has no value
			continue#ignore it
		
		if "gloss" not in token:#if the gloss(key) has no value
			token["gloss"] = ""#put an empty string as the value
		
		if "pos" not in token:#if the pos (key) has no value
			token["pos"] = "not.sure"#say not sure
			
		out_list.append(token)
	return(out_list)

isaan_dict_list = Isaan_xml_dicter(corpus_text) #this line create a dictionary of word, gloss and part of speech 
#print(isaan_dict_list[:50])

#extracting and merging word and gloss for concordances
def extract_value(list_dict):
	out_list = []
	for token in list_dict:
		pair = token["word"] + "_" + token["gloss"]
		#for k,v in token.items():
		out_list.append(pair)
	return(out_list)

isaan_word_list = extract_value(isaan_dict_list) #this line create a list of words_gloss to be used for concordance analysis
#print(isaan_word_list[:50])

#generating concordances using the following functions
#you can modify tokenize function to ignore certian symbols

def tokenize(input_list):
	tokenized = []
	punc_list = [",_", "[_", "]_", "§_"]
	for x in input_list:
		if x not in punc_list:
			tokenized.append(x)
	return(tokenized)

isaan_word_gloss = tokenize(isaan_word_list)
#print(isaan_word_gloss[:50])

#generating concordances
#this series of codes is credited to Dr. Kristopher Kyle see https://github.com/kristopherkyle/Corpus-Methods-Intro
import random
random.seed(1) #set seed

def concord(tok_list,target,nleft,nright):
	hits = [] #empty list for search hits

	for idx, x in enumerate(tok_list): #iterate through token list using the enumerate function. idx = list index, x = list item
		if x in target: #if the item matches one of the target items

			if idx < nleft: #deal with left context if search term comes early in a text
				left = tok_list[:idx] #get x number of words before the current one (based on nleft)
			else:
				left = tok_list[idx-nleft:idx] #get x number of words before the current one (based on nleft)

			t = x #set t as the item
			right = tok_list[idx+1:idx+nright+1] #get x number of words after the current one (based on nright)
			hits.append([left,t,right]) #append a list consisting of a list of left words, the target word, and a list of right words

	return(hits)

#writing concordances files
def write_concord(outname, conc_list):
	outf = open(outname,"w", encoding ="utf8")
	outf.write("left_context\tnode_word\tright_context") #write header (optional)
	for x in conc_list:
		left = " ".join(x[0]) #join the left context list into a string using spaces
		target = x[1] #this is the node/target word
		right = " ".join(x[2]) #join the left context list into a string using spaces
		outf.write("\n" + left + "\t" + target + "\t" + right)
	outf.flush()
	outf.close()

####concordance for ʔaw 'take'
#concordances with the § which will help with the clausal boundary analysis
take_conc = concord(isaan_word_list, ["เอา_take"],5,5)
write_concord("take_concordances_final.tsv", take_conc)
